{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c34cb25-109f-442a-bf07-635f50a34aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles with sentiment saved to finance_articles_with_sentiment.xlsx\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "from textblob import TextBlob\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "import schedule\n",
    "import time\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_finance_news(tickers: list) -> pd.DataFrame:\n",
    "    news_data = []\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # Use feedparser to get news headlines from Yahoo Finance RSS feed\n",
    "        feed_url = f'https://feeds.finance.yahoo.com/rss/2.0/headline?s={ticker}&region=US&lang=en-US'\n",
    "        NewsFeed = feedparser.parse(feed_url)\n",
    "\n",
    "        for feed in NewsFeed.entries:\n",
    "            headline = feed.title\n",
    "            link = feed.link\n",
    "            published_date = feed.published\n",
    "            # Download and parse the article\n",
    "            response = requests.get(link, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            description = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "            if not description:\n",
    "                description = soup.get_text()\n",
    "            \n",
    "\n",
    "          \n",
    "            # Parse published date to datetime format (assuming a consistent format)\n",
    "            try:\n",
    "                datetime_obj = datetime.strptime(published_date, '%a, %d %b %Y %H:%M:%S %z')\n",
    "                formatted_date = datetime_obj.strftime('%Y-%m-%d')\n",
    "                formatted_time = datetime_obj.strftime('%H:%M:%S')\n",
    "            except ValueError:\n",
    "                # Handle potential parsing errors with a generic format or logging\n",
    "                formatted_date = None\n",
    "               \n",
    "           \n",
    "\n",
    "            now=datetime.now()\n",
    "            today = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            news_data.append({\n",
    "                'Stock Identifier': ticker,\n",
    "                'Headline': headline,\n",
    "                'Link': link,\n",
    "                'Published Date': formatted_date,\n",
    "                'Published Time': formatted_time,\n",
    "                'Description': description,\n",
    "                'Last Run Time' : today, \n",
    "            })\n",
    "\n",
    "            # Add a delay between requests to avoid rate limiting\n",
    "            #time.sleep(1)\n",
    "\n",
    "    # Combine scraped data from RSS and web scraping (if implemented)\n",
    "    df = pd.DataFrame(news_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tickers = ['AAPL', 'GOOG', 'AMZN', 'MSFT', 'TSLA']  # Your desired tickers\n",
    "    df = get_finance_news(tickers)\n",
    "\n",
    "    # Save DataFrame to Excel\n",
    "    df.to_excel('finance_articles_with_sentimentunique.xlsx', index=False)\n",
    "\n",
    "    print(\"Articles with sentiment saved to finance_articles_with_sentiment.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bf8bbb-b5bd-45b5-972b-cf9b32de7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to MongoDB\n",
    "client = MongoClient('mongodb+srv://team:JtRbTot4ERYO4acZ@cluster0.qewkk2p.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0')\n",
    "db=client['Capstone']\n",
    "collection = db['Headlines with Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2320f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the combined data into MongoDB\n",
    "records = df.to_dict(orient='records')\n",
    "# Loop over each record\n",
    "for record in records:\n",
    "    # Check if the headline already exists in the collection\n",
    "    if collection.count_documents({'Headline': record['Headline']}, limit = 1) == 0:\n",
    "        # If it does not exist, insert the record\n",
    "        try:\n",
    "            collection.insert_one(record)\n",
    "        except errors.DuplicateKeyError:\n",
    "            # This could occur if another process inserted the same record in the meantime\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60dcaf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles with sentiment saved to MongoDB and finance_articles_with_sentiment.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Print a success message\n",
    "print(\"Articles with sentiment saved to MongoDB and finance_articles_with_sentiment.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
